# ğŸš¦ ADRL Traffic Signal Control  
### Adaptive Deep Reinforcement Learning for Smart Traffic Optimization

This project implements an **Adaptive Deep Reinforcement Learning (ADRL)**â€“based traffic signal controller using **D3QN (Double Deep Q-Network + Dueling + Prioritized Replay)** to optimize traffic flow at intersections.  
The system interacts with a simulation environment (SUMO or custom), observes traffic states, and learns an optimal traffic light policy to reduce congestion and waiting times.

---

## ğŸš€ Features

- **D3QN Agent** with:
  - Double Q-Learning  
  - Dueling Architecture  
  - Prioritized Experience Replay  
- **SUMO-based traffic simulation** (optional)  
- **Custom Python environment** for testing RL logic  
- Dynamic traffic phase switching  
- Logging and performance evaluation  
- Easily extendable for multi-intersection control  

---

## ğŸ“ Project Structure

ADRL_Traffic_Signal_Control/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ agent.py # RL agent (D3QN)
â”‚ â”œâ”€â”€ networks.py # Neural network architectures
â”‚ â”œâ”€â”€ replay.py # Prioritized replay buffer
â”‚ â”œâ”€â”€ env_sim.py # Custom traffic simulation
â”‚ â”œâ”€â”€ env_sumo.py # SUMO environment interface
â”‚ â”œâ”€â”€ train.py # Training loop
â”‚ â”œâ”€â”€ evaluate.py # Evaluation scripts
â”‚ â””â”€â”€ utils.py # Helper utilities
â”‚
â”œâ”€â”€ data/ # Optional traffic logs or configs
â”œâ”€â”€ models/ # Saved trained models
â”œâ”€â”€ runs/ # Training logs, plots
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Documentation

yaml
Copy code

---

## ğŸ§  Algorithm Used â€” D3QN

The project uses a hybrid of improvements on the standard DQN:

âœ” **Double DQN**  
âœ” **Dueling Networks**  
âœ” **Target Networks**  
âœ” **Prioritized Experience Replay**  

These upgrades help the agent:

- Learn stable policies  
- Avoid overestimation  
- Converge faster  
- Handle large state spaces  

---

## ğŸ§ª How Training Works

1. Environment provides traffic state:
   - Vehicle counts  
   - Waiting time  
   - Queue length  
2. Agent selects an action:
   - Switch phase  
   - Extend green  
   - Change to next cycle  
3. Model receives reward:
   - Minimize queue length  
   - Minimize delay  
4. Agent updates Q-values  
5. Over many episodes, the system learns to **optimize traffic flow**  

---

## ğŸ“¦ Installation (Local)

```bash
git clone https://github.com/RamakantKhasnis/ADRL_Traffic_Signal_Control.git
cd ADRL_Traffic_Signal_Control
pip install -r requirements.txt
â–¶ï¸ Usage
Train the model
bash
Copy code
python src/train.py
Evaluate trained model
bash
Copy code
python src/evaluate.py
ğŸ“Š Results (Expected)
Reduced total waiting time

Reduced queue length

Faster intersection clearing

More stable traffic flow

Performance varies depending on the traffic pattern and SUMO configuration.

ğŸ“ Notes
SUMO installation is optional; custom Python env included.

This project is GPU-ready if using PyTorch.

All code is modular and extendable for multi-intersection traffic networks.

ğŸ‘¨â€ğŸ’» Author
Ramakant Khasnis
Deep Learning | Reinforcement Learning | Computer Vision

